[
  {
    "path": "posts/2024-08-01-the-attention-economy/",
    "title": "The Attention Economy: How Incentive Structures Drive Division",
    "description": "Bad actors are often blamed for sowing ideological division. But what about the incentive structures that underlie our media landscape?",
    "author": [],
    "date": "2024-11-01",
    "categories": [],
    "contents": "\n\nContents\nThe Attention Economy\nMethod 1: Shift the emotional tone of content\nMethod 2: Shift the partisan identity of content\nOutlining and fixing the problems\n\n\n\nIn the lead-up to the 2020 election, 87% of Democratic Party voters expected a Biden victory, while 84% of Republicans expected Donald Trump to win (Coibion, Gorodnichenko, and Weber 2020). In fact, as many as 15% of Democrats and 23% of Republicans rated their candidate’s chances of winning at 100%. While it’s not wholly unusual for supporters of a particular candidate to overestimate their likelihood of success, this partisan division effectively amounts to two groups experiencing entirely different realities. These disparities make it easier for baseless conspiracies about election fraud to gain traction over the truth. This growing ideological rift among the American populace is now rightly seen as a catalyst for political violence, eroding trust in democratic institutions and threatening the very fabric of democratic governance.\nWhen an event like January 6th occurs, there’s an immediate temptation to individualize the blame. We blame the rioters for their failure to distinguish basic facts from outrageous lies and we blame Donald Trump and his allies for spreading those lies. However, events like this are only made possible by the substantial epistemic divide in Americans’ beliefs. While Trump may have armed the bomb and the rioters flipped the switch, it was the distributors of pervasive misinformation and deep-seated polarization that built the bomb itself. That can only lead us to wonder, who are the architects of these ideological divisions?\nIn tracing the root causes of polarized beliefs, attention often falls on the increasingly partisan coverage of mainstream news outlets (Kim, Lelkes, and McCrain 2022). This raises a chicken-or-egg question: are news networks responding to consumer demand by adjusting their offerings to match the beliefs of their prospective audiences? Or, is the divide in the audience’s beliefs being driven by the increasingly partisan coverage offered by news networks?\nTo understand the possible reasons behind the shift towards more partisan coverage, we need to break down the media landscape and the incentive structures that led us to this point. As Charlie Munger put it: “Show me the incentives, and I’ll show you the outcome”\nThe Attention Economy\nIf everyone was after land in the 19th century and oil in the 20th, it seems that Beanie Babies / NFTs / attention is the new hottest commodity on the market.\nThis shift towards capturing attention has brought with it a slew of new challenges. Where tangible resources like oil allow us to leverage our extensive knowledge of fluid dynamics and geology to maximize their extraction, the mechanisms underlying information demand are far more elusive. The complexity of these dynamics leave us without an optimized method to attract attention, making approximation and innovation key to winning eyeballs.\nPicture yourself as the CEO of a media conglomerate in the early 2000s. Your profits are dependent on the share of the market you hold, but that market share is constantly being siphoned off by competitors with similar offerings. The functional features that your network once prided itself on—being first on the scene or having exclusive coverage—begin to dry up as the internet democratizes the information space. You’re left asking yourself:\n\nHow can I shore up a dedicated audience that I can rely on to maintain my network’s existence?\n\n\n \n\nMethod 1: Shift the emotional tone of content\nIt’s no industry secret that emotive content gets views. Emotion plays a central role in how we select which content to consume, and in turn, how that content is curated. In psychology, we think of emotions as occurring across two dimensions: valence (how positive or negative the emotion is) and arousal (how intense an emotion is). For example, excitement is an example of an emotion that is high in both valence and arousal, while a feeling of calm would be high in valence but low in arousal.\nIn an analysis of online news headlines between 2000 and 2019, Rozado, Hughes, and Halberstadt (2022) reported a gradual but sustained trend towards more negative emotional valence. While they didn’t explicitly analyze trends in the emotional arousal of content, the frequency of headlines with a neutral emotional payload dropped from around 70% to below 50%, while those featuring anger and fear increased by 100% and 150%, respectively1.\n\n\n\nIf we combine the sentiment analysis of Rozado, Hughes, and Halberstadt (2022) with a metric of partisan bias (we’ll use the scores from the August 2024 [v12.0] Ad Fontes media bias report), we find that it is more biased news outlets that tend to produce the most negatively-valenced content2.\n\n\n\n\n\nFrom these data, it seems that if you want to produce more emotive content, it might be necessary to adopt a more strongly partisan stance3. It’s plausible to suggest, therefore, that the shift towards more biased content may simply be a side-effect of the pursuit of emotionally-driven eyeballs.\nMethod 2: Shift the partisan identity of content\nSimilar to emotive content, it’s a well-known fact of human information demand that we tend to favor information that is consistent with our existing beliefs or values (commonly termed confirmation bias). Picture someone whose conspiratorial anti-establishment beliefs are an important part of their identity. This person is more likely to seek out anti-vax information, for example, than someone who doesn’t share those beliefs. This behaviour is commonly cast as a defense mechanism for protecting our chosen identity – we avoid information that we think might challenge our beliefs because the dissonance between the two is unpleasant. However, this view of confirmation bias is reductive and is not consistent with our understanding of how we select information to sample and how we use that information to inform ourselves.\nTo an extent, behavior consistent with confirmation bias is not biased at all, but a logical, Bayesian process of integrating one’s priors into the evaluation of new evidence. To someone with a deep mistrust of institutions, anti-vax conspiracies are far more plausible than they are to the average person. Extend this to a less extreme example and we can understand that someone with right-leaning beliefs might choose to watch Fox News over a more balanced alternative simply because they believe it to be more accurate than a center-aligned network. All of this is to make the obvious point that our beliefs and our decisions about where we choose to get information from are not independent.\nThe evolution of one’s beliefs and one’s choice of media content can be thought of as a spiral (Slater, Shehata, and Strömbäck 2020). By integrating our existing beliefs and attempting to avoid cognitive dissonance, we sample media that is consistent with our existing values and beliefs. The sampled media content in turn reinforces those pre-existing beliefs and values, resulting in deeper entrenchment and resilience to change. This produces a feedback loop, as the strengthened beliefs then influence media selection which then continues reinforcing the increasingly entrenched beliefs…and so on…\nSo, how might you—early 2000s media mogul—leverage these dynamics to build a dedicated following for your network? Let’s imagine that your news network currently operates as a center-right news outlet – considered the most right-wing of the mainstream outlets. You might consider two main options:\n\n \n\n\nOption 1: ConvergeMove your network’s coverage closer to the ideological center\n\n\n \n\nThis option might allow you to expand your network of likely viewers by expanding your appeal to an audience with more center-aligned beliefs.\n\n \n\n\n \n\n\nOption 2: DivergeMove your network’s coverage further away from the ideological center.\n\n\n \n\nThis option might increase the ideological distance between your existing audience and the other networks, allowing you to create and corner a less competitive share of the market.\n\nTo get a sense of how these options might work, we can conduct a small-scale simulation. In our simulation, individual agents will repeatedly sample from five media outlets, obtaining some information and updating their beliefs and their trust in each of the five outlets accordingly. This simulation is adapted from the work of Perfors and Navarro (2019) – I’ll leave more details in a footnote here4.\nTo test how changing the partisan identity of your outlet’s coverage might affect audience metrics, I simulated the choices of 1200 Bayesian, rational agents across 400 iterations. In the first and last 100 iterations, the media outlets did not change in the bias of their coverage. In the middle 200 iterations, Outlet E steadily shifted the partisan identity of its coverage towards/away from the others. Below are example simulations of 30 agents and 5 media outlets. The axes here are fairly arbitrary—we would need more dimensions to capture real ideological positions—we’re just demonstrating here that it’s possible to simulate the basic dynamics.\n\n\n\n\n\n\n\n\n\n\nSo, which option will give your outlet better long-term outcomes? Let’s consider two important metrics: (1) What proportion of all views does your outlet receive, and (2) How many dedicated viewers does your outlet have?\nTo test these, we’ll focus on the final 100 iterations of the simulations. I’m defining a dedicated viewer as any agent who samples from a single outlet on at least 80 of the last 100 iterations.\n\n\n\nOverall, it looks like diverging away from the ideological center produces the best results for Outlet E. Specifically, while diverging didn’t increase the total proportion of views Outlet E received, it did increase its number of regular, dedicated viewers by 24% relative to the converge option.\nOutlining and fixing the problems\nIn this article, I deliberately chose not to address certain incentives that could motivate media outlets to propagate fringe or extremist ideologies. Specifically, I overlooked the potential for these outlets to divert attention from class disparities by focusing on divisions related to race, gender, or sexuality—a strategy that could disproportionately benefit the wealthy owners and financiers of these media entities. Despite taking this charitable approach, we still found that an attention-based economy alone contains potential incentives that promote divisive ideological shifts. Specifically, we found that, in pursuit of a dedicated audience, steadily shifting towards the production of content that is emotive and ideologically extreme could be an effective strategy.\nUltimately, there is no panacea to address the wide-ranging impact our attention-based economy has had on media divisions. A collective approach that combines regulatory oversight, education, ethical journalism, and technological refinement offers the best chance for reducing media-driven ideological polarization. Only through sustained efforts across these domains can we hope to rebuild a media landscape that fosters informed dialogue and promotes societal unity rather than division.\n\n\n\n\n\n\n\n\n\n\n\n\nCoibion, Olivier, Yuriy Gorodnichenko, and Michael Weber. 2020. “Political Polarization and Expected Economic Outcomes.” Working Paper 28044. Working Paper Series. National Bureau of Economic Research. https://doi.org/10.3386/w28044.\n\n\nKim, Eunji, Yphtach Lelkes, and Joshua McCrain. 2022. “Measuring Dynamic Media Bias.” Proceedings of the National Academy of Sciences 119 (32): e2202197119. https://doi.org/10.1073/pnas.2202197119.\n\n\nPerfors, Andrew, and Danielle J Navarro. 2019. “Why Do Echo Chambers Form? The Role of Trust, Population Heterogeneity, and Objective Truth.” In CogSci, 918–23.\n\n\nRozado, David, Ruth Hughes, and Jamin Halberstadt. 2022. “Longitudinal Analysis of Sentiment and Emotion in News Media Headlines Using Automated Labelling with Transformer Language Models.” PLOS ONE 17 (10): 1–14. https://doi.org/10.1371/journal.pone.0276367.\n\n\nSlater, Michael D., Adam Shehata, and Jesper Strömbäck. 2020. “Reinforcing Spirals Model,” 1–11. https://doi.org/https://doi.org/10.1002/9781119011071.iemp0134.\n\n\nA plausible explanation for the change in mean valence could be the advent of more partisan outlets with strong, negative emotional content in the middle of 2000-2019 time period (e.g., The Daily Wire). However, the downward trend in mean valence is only slightly lessened when we exclude the 27 outlets that don’t have data for the full 20-year period.↩︎\nNote that all the content in these analyses is based on written (online) news media. This means that the scores for outlets with both written and broadcast content (e.g., Fox News, MSNBC, etc.) are based solely on their written content. This is relevant because the broadcast content for some of these outlets is far more focused on opinion pieces, which tend to have a considerably stronger partisan bias and lower reliability (e.g., primetime shows from the likes of Laura Ingraham, Sean Hannity, Jesse Watters, or Lawrence O’Donnell)↩︎\nThis analysis is both cross-sectional and correlational, so take it with a grain of salt.↩︎\nIn brief, each agent in the simulation is represented by their current belief state and their level of trust in each of the five available media outlets. In each iteration, the agents each choose one of the media outlets to tune in to. The more they trust an outlet (a function of how closely the outlet’s views align with their own), the more likely the agent is to sample from it. The media outlet produces an information sample drawn from a normal distribution around the mean of the outlet’s coverage. Using a a probabilistic acceptance rule (Metropolis-Hastings algorithm), the agent may accept the new information and update their beliefs, allowing for exploration of different belief states. The agent’s trust in each outlet is then re-calculated using the likelihood of the recent samples they received from each source given their current belief-state.↩︎\n",
    "preview": "posts/2024-08-01-the-attention-economy/attentionEcon.png",
    "last_modified": "2024-11-03T18:10:17-05:00",
    "input_file": {},
    "preview_width": 1792,
    "preview_height": 1024
  },
  {
    "path": "posts/2024-10-10-dondpart3/",
    "title": "Deal or No Deal | Part 3: Becoming the Banker",
    "description": "Is Deal or No Deal's Banker giving out more money than they need to?",
    "author": [],
    "date": "2024-10-01",
    "categories": [],
    "contents": "\n\nContents\nSimulating contestants\nBecoming the Banker\nExploiting subjective utility\nIndividualizing offers\nThat’s a wrap\n\n\n\nThis is the final installment in a series of blog posts exploring the mechanics of TV game show Deal or No Deal. In part 1, we examined how The Banker determines how much to offer, followed by part 2, where we explored how contestants evaluate whether to accept or reject these offers.\nIn this third and final chapter, we’ll be seeing whether The Banker can leverage what we’ve learned to minimize the payouts of the game.\nSimulating contestants\nBefore we can design our own algorithm for producing bank offers, we’ll first need to replicate the performance of The Banker. To achieve this, I’ll be using the parameters we estimated for both The Banker and the contestants to simulate a new batch of contestants – you can read more about those parameters in the previous two parts.\nI simulated 1000 new contestants using this method. To verify that our simulation is roughly equivalent to the real games, I took 1000 bootstrap samples of 145 contestants (the sample size of the original dataset) to find the expected mean winnings of the contestants. Below, I’ve plotted the 95% highest density interval, with lines showing the median of the simulated samples and the observed mean of our original dataset (Post et al. 2008). It looks like the true mean was in our plausible range (albeit at the lower end), which is a good indicator that our simulation is approximating the contestants’ performance.\n\n\n\nFor a secondary check, I also plotted the number of the 1000 simulated contestants that accepted The Banker’s offer each round. Similar to our dataset, acceptance rates are low in the early rounds, but peak around round 6 before steadily declining:\n\n\n\nBecoming the Banker\nSo, we have a benchmark to aim for—our next goal is to design a Banker that will outperform the current one, paying out less by being more strategic in the offers it makes. First, let’s set out some ground-rules:\nThe distribution of accepted offers in each round should stay roughly the same. Our banker still has to produce broadcastable games – if all of the round 2 offers are accepted, that won’t make for very interesting TV.\nThe Banker still has to be broadly predictable. The modelling approach we’re using relies on the contestants to approximate the offers they will receive in the next round. We don’t want to improve performance by generating a mismatch between the contestants’ expectations and the offers—that wouldn’t be a realistic long-term strategy. Our banker will follow the same rough trajectory as the original, increasing the proportion of the Expected Value it offers each round.\nExploiting subjective utility\nLet’s start with something simple – contestants are willing to accept well below the Expected Value of the remaining cases. Despite this, The Banker starts out by offering a small percentage of the Expected Value, but quickly increases to approach the full Expected Value in the final few rounds. Instead, we could consider a banker that approaches a different limit—one a bit below the full expected value.\nTo put it formally, we’ll test how changing \\(\\gamma\\) in the equation below affects behavior. Here, \\(\\gamma\\) effectively dictates the maximum proportion of the Expected Value The Banker will offer. In the standard version of the model, \\(\\gamma = 1\\):\n\\[\\begin{equation}\n\\tag{1}\nb_r = b_{r-1} + (\\gamma - b_{r-1})\\cdot\\rho^{9-(r-1)}\n\\end{equation}\\]\nIt’s important we bear in mind that our model assumes the contestant is myopic: when deciding whether to continue playing, they only consider the expected offer from the next round. However, if we put a cap on those offers, we might be inadvertently exploiting this myopia and encouraging our simulated contestants to accept low offers by reducing the expected next-round offer. To ensure this is not the case, I allowed the contestant to consider not only the next round, but also the final round (the contents of their own case) when deciding whether to continue (i.e., we compute the subjective value of the expected next-round offer AND the subjective value of the possible contents of the final case. The No Deal value is the maximum of the two).\nSo, how does altering \\(\\gamma\\) affect the results? Below I’ve plotted the simulated winnings of 1000 bootstrap samples of 145 contestants each for \\(\\gamma = 1\\) against those produced from different levels of \\(\\gamma\\). I’ve annotated the median average winnings for each condition, which can be compared to the \\(\\gamma = 1\\) median of $97,247:\n\n\n\nSo, it seems that, if we reduce \\(\\gamma\\), we reduce contestants’ winnings quite substantially—as much as 33% when we reduce \\(\\gamma\\) down to 0.7.\nWe still need to make sure that we’re not breaking our rules, though – these games have to produce broadcastable TV. Let’s take a look at the distribution of rounds in which these simulated games ended, again comparing to the standard \\(\\gamma = 1\\) condition. I’ve annotated the average length of the games in each condition, which can be compared to the \\(\\gamma = 1\\) condition average of 6.03…\n\n\n\nFrom these plots we can see that the general pattern of rejecting early offers and accepting those in the middle rounds replicates through most of our variations. When we bring \\(\\gamma\\) down to 0.6, we notice a red flag starting to emerge: many contestants show a preference for holding out to the final round, where they receive the full value of the last remaining case.\nGiven these results, it seems as though lowering \\(\\gamma\\) down as far as 0.7 is potentially feasible, reducing winnings by one third while only shortening the average game by 0.31 rounds. For the rest of the analyses, we’ll stick with a slightly more conservative \\(\\gamma\\) of 0.8.\nIndividualizing offers\nRecall how we determine how much to offer the contestant each round. We take the proportion of the Expected Value that we offered in the previous round and we increment it by some proportion of the difference between that proportion and \\(\\gamma\\). The speed at which the proportion offered approaches \\(\\gamma\\) is dictated by \\(\\rho\\). The Banker uses a \\(\\rho\\) of around 0.787, and adds random noise to make sure the offers are variable:\n\\[\\begin{equation}\n\\tag{1}\nb_r = b_{r-1} + (\\gamma - b_{r-1})\\cdot\\rho^{9-(r-1)}\n\\end{equation}\\]\nHowever, as we learned in the previous posts, two contestants facing the same decision may evaluate it differently due to the way they calculate the reference point against which they compare potential outcomes.\nIn the early rounds, we can leverage these differences by giving better offers to the contestants with the biggest gap between their Deal! Value and the Expected Value of the remaining cases.\nSo, rather than varying \\(\\rho\\) randomly for each of the 1000 simulated contestants, on each round, we:\nCalculated the expected Reference Point by assuming the median parameter values, then used that to calculated the expected Deal! Value (i.e., the monetary value at which we expect the contestant to have a 50/50 preference for accepting/rejecting).\nCalculated z-scored residuals when regressing the Expected Value on the expected Deal! Value.\nTransformed those z-scores into adjustment values by scaling them down, then exponentiating the result. The adjustment values were scaled in a linearly decreasing fashion, starting from .05 and finishing at 0.\nMultiplied the \\(\\rho\\) of each contestant by the resultant adjustment value.\nUsing z-scores allows us to keep the mean \\(\\rho\\) the same—we just employ a more targeted approach to making our offers1. Put simply, we give better early-round offers to contestants our model suggests have a larger difference between the true Expected Value of their remaining cases and their subjective minimum acceptable offer.\nWe can compare the performance of this updated Banker to that of the regular Banker, both using a \\(\\gamma\\) value of 0.8:\n\n\n\nBy strategically adjusting their offers, the updated Banker produced a further reduction in average winnings of $10,000 without meaningfully reducing the average duration of the games.\nBear in mind that this is just a proof-of-concept—these parameters aren’t necessarily optimized for reducing payouts. If we wanted to find the optimized parameters for minimizing winnings, we could define guidelines around the required distribution of accepted rounds and enter weights for the various candidate variables and their interactions.\nThat’s a wrap\nAnd so ends our journey through the mechanisms of Deal or No Deal.\nThe Banker may be a ruthless capitalist but they’re not a particularly good one, as our analyses suggest they’re overpaying the average contestant by somewhere around 25-30%. By implementing some minor changes that reduced the average game duration by as little as 0.2 rounds (~3%), our Banker cut median winnings down from $97k to just $73k.\nWhile some pilot testing would help to confirm the feasibility of these changes and more fine-tuning could benefit the exact parameters of the new Banker’s algorithm, it seems The Banker might benefit from acknowledging the more human aspects of contestants’ decisions.\n\n\n\nPost, Thierry, Martijn J van den Assem, Guido Baltussen, and Richard H Thaler. 2008. “Deal or No Deal? Decision Making Under Risk in a Large-Payoff Game Show.” American Economic Review 98 (March): 38–71. https://doi.org/10.1257/aer.98.1.38.\n\n\nFor a functional algorithmic Banker, we wouldn’t be able to rely on z-scores, as we wouldn’t be running 1000 games simultaneously in the real gameshow. However, we could estimate the intercept and slope of the regression alongside the variance of the residuals at each round provided we simulated enough contestants or simulated all possible outcomes.↩︎\n",
    "preview": "posts/2024-10-10-dondpart3/theBanker.png",
    "last_modified": "2024-11-05T09:36:53-05:00",
    "input_file": "dondpart3.knit.md",
    "preview_width": 1792,
    "preview_height": 1024
  },
  {
    "path": "posts/2024-08-03-dondpart2/",
    "title": "Deal or No Deal | Part 2: Pushing the Button",
    "description": "Pressure, probability, and risk — how does this confluence of factors determine the decisions made by contestants in Deal or No Deal?",
    "author": [],
    "date": "2024-09-01",
    "categories": [],
    "contents": "\n\nContents\nWhat does an acceptable offer look like?\nDecisions, decisions…\nSubjective value\nReference points\nPredicting the Future\nPutting it all together\nUntil next time…\n\n\n\nIn this series of blog posts, we’re exploring the mechanics of TV game show Deal or No Deal. In part 1, we examined how The Banker determines how much to offer. This time, we’ll be turning our attention to the contestant – how do they decide when to accept and when to reject The Banker’s offer?\nWe’ll start by exploring the offers that real contestants accepted. From there, we’ll replicate the analyses performed by Post et al. (2008), elaborating on the reasoning behind their model and exploring possible alternatives.\nWhat does an acceptable offer look like?\nLast time, we learned that The Banker will offer extreme lowballs in the early stages of the game, steadily improving their offers as the round progress. With that in mind, let’s take a look at the rounds in which contestants accept The Banker’s offer…\n\n\n\nAs expected, not a single contestant accepts an offer made in the first two rounds of the game. In fact, most hold their nerve until at least round 6 before accepting an offer, with as many as 15 rejecting all offers and taking home the value of their selected case.\nSo what does that mean for the value of the offers that are accepted? Let’s have another look at the comparison between The Banker’s offers and the Expected Value (average amount in the remaining cases), this time dividing the offers between rejected and accepted…\n\n\n\n\n\nUnsurprisingly, most of the extremely poor offers are rejected, with the accepted offers tending to align more closely with the true Expected Value of the remaining cases. However, it’s also apparent that contestants are often willing to accept well below the Expected Value—on average, cheating themselves out of substantial sums—why?\nIn order to understand these decisions, we’ll first need to break them down to their constituent parts…\nDecisions, decisions…\nEach round, contestants are presented with two options: accept the offer (Deal!) or keep on playing (No Deal). To decide between the two, they must assign a value to each and then compare the two. This is the logic followed by Post et al. (2008), who suggest that the probability an offer will be accepted is a function of the Deal! value minus the No Deal value.\nThe Deal! value is straightforward – it represents the subjective value (explained further below) of the current offer.\nTo determine the No Deal value, Post et al. (2008) suggest that contestants mentally project themselves one round into the future and estimate what the offer in the next round might be. The No Deal value is computed as the average subjective value of those estimated next-round offers.\nSo, if you think the offer you receive next round will, on average, be higher than the current offer, you reject the current offer—easy, right! Well, almost…\nSubjective value\nOne complication arises when we consider that the subjective value of gaining and losing money is not related linearly to the objective value of that money. There are two key ways in which subjective value and objective value differ:\nDiminishing Returns: The subjective value increases with actual gains, but at a decreasing rate. For example, the subjective difference between winning $100 versus $0 is more significant than between $200 and $100.\nUnequal Impact of Losses vs. Gains: Losses impact us more than equivalent gains. The discomfort of losing $50, for instance, might outweigh the pleasure of gaining $50.\nTo calculate this subjective value, we can turn to the prospect theory value function (Tversky and Kahneman 1992):\n\\[\\begin{equation}\n\\tag{1}\nSV(x \\mid RP) =\n\\begin{cases}\n-\\lambda(RP - x)^\\alpha & \\text{if } x \\leq RP \\\\\n(x - RP)^\\alpha & \\text{if } x > RP\n\\end{cases}\n\\end{equation}\\]\nTypically, \\(\\alpha\\) values lie below 1, producing decelerating functions in both gains and losses. This means that the function accurately reproduces the diminishing returns we mentioned earlier. It is also standard to find \\(\\lambda > 1\\), producing a function that is steeper for losses than gains. This allows the function to capture the relatively larger impact of gains than losses. In this function, \\(RP\\) refers to the reference point, which represents the value where the psychological impact is neutral – perceived as neither a gain nor a loss.\nLet’s see what this function might look like with standard values of \\(\\lambda = 2.25\\), \\(\\alpha = 0.88\\), and a Reference Point of \\(0\\):\n\n\n\nReference points\nBut, you might have noticed, in the game of Deal or No Deal the contestant can only win money, never lose it—surely anything below $0 won’t be relevant, here? Not quite! Remember that subjective value is determined with respect to some Reference Point (RP). While it might seem intuitive to use a Reference Point of 0, Post et al. (2008) suggest that the reference point for a contestant in Deal or No Deal starts at the average Expected Value of all possible outcomes (i.e., the average case value: $131,477.50 in the US edition).\nTo give this logic a sanity check, consider this: if you were given the opportunity to participate in Deal or No Deal and ended up taking home $500, how would you feel? In this scenario, you would end the game $500 richer than before the game started, but you would probably feel disappointed with the result. This is because $500 is well below the reference point we started at – even though you won $500, it would feel as though you lost.\nPost et al. (2008) argue that contestants update their Reference Point as the game progresses. They suggest that, in a given round, the Reference Point is dictated primarily by The Banker’s offer for that round: \\(B(x_r)\\). Also contributing to the Reference Point are the difference between the Expected Value of the remaining cases and the initial Expected Value of all cases: \\(d_r^{(0)}\\). Finally, the Reference Point depends on the previous occurrences in the game, operationalized as the difference between the current Expected Value and the Expected Value from two rounds ago: \\(d_r^{(r-2)}\\)1. Putting that all together:\n\\[\\begin{equation}\n\\tag{2}\nRP(r) = (\\theta_1 + \\theta_2 \\cdot d_r^{(r-2)} + \\theta_3 \\cdot d_r^{(0)}) \\cdot B(x_r)\n\\end{equation}\\]\nOne interesting implication of this moving reference point is that allows contestants to judge a decision not only by the options presented to them, but also by the decisions they faced en-route to the current situation…\nFor example, imagine two contestants in the final round. Both contestants face the exact same decision, with the $100 case and the $50,000 remaining. In the last two rounds, Contestant 1 eliminated the $1 case and the $10 case, bringing their Expected Value up substantially. Contestant 2, on the other hand, just eliminated the $25,000 and $50 cases, leaving their Expected Value pretty much the same as it was. Given these differences, Contestant 1 will have a lower reference point than Contestant 2, as the reference point gravitates towards the Expected Value of previous rounds. As a consequence, Contestant 2 is more likely to continue playing than Contestant 12, despite being faced with exactly the same decision.\nPredicting the Future\nSo far, we have established a descriptive account of contestants’ decisions and specified a function to model the subjective value of Bank offers. The final step for putting together a model of decision-making in Deal or No Deal is to make clear our assumptions about the contestants. Earlier, I mentioned that the No Deal value requires contestants to mentally project themselves forward one round to estimate what their next round offer might be.\nTo do this, we must first assume that the contestants have some mental model of how the Bank offers are computed. It’s fair to assume that contestants are not naive to the increasing proportion of the Expected Value that The Banker offers each round. Taking the equation we used to estimate bank offers in part 1, we find that bank offers for the next round can be derived by:\n\\[\\begin{equation}\n\\tag{3}\nb_{r+1} = b_{r} + (1 - b_{r})\\cdot\\rho^{9-r}\n\\end{equation}\\]\nPost et al. (2008) assumed that participants accurately approximate the true value of \\(\\rho\\). This isn’t necessarily true, though – we’ll leave \\(\\rho\\) as a free parameter to see if a different value fits the data better.\nFinally, we have to make an assumption about how contestants project into the future – which potential outcomes do they anticipate? In the modelling account of Post et al. (2008), they assumed contestants mentally simulate all possible outcomes for the next round, estimating the bank offer and computing the subjective value for each. This seems fairly implausible given the sheer number of possible outcomes each round, as well as being computationally impractical for our modelling approach. To make this more manageable, we’ll assume instead that they only simulate 20 possible future states.\nPutting it all together\nThat’s a lot to keep in mind! Let’s summarize everything we’ve outlined so far…\nEach round, the contestant receives an offer. They also mentally project themselves one round into the future, estimating the offers they might receive in the next round. Both the current offer and these potential future offers are transformed using a value function that adjusts for risk preferences and a dynamically updating reference point. To predict the decisions, we use the difference between the subjective value of the current offer (the Deal! value) and that of the expected next-round offer (No Deal value):\n\\[\\begin{equation}\n\\tag{4}\n\\Delta = Deal - No Deal\n\\end{equation}\\]\nWe then feed this difference into a logit function to compute the probability (\\(p\\)) of accepting the current offer:\n\\[\\begin{equation}\n\\tag{5}\np = \\sigma(\\frac{\\Delta}{\\tau}) = \\frac{1}{1 + e^{-\\frac{\\Delta}{\\tau}}})\n\\end{equation}\\]\nThe temperature parameter (\\(\\tau\\)) influences the determinism of the decision: a lower \\(\\tau\\) makes decisions more sensitive to differences in value, emphasizing a more deterministic choice pattern. The higher the subjective value of the current offer relative to the average subjective value of estimated future offers, the more likely it is that the current offer will be accepted.\nFor simplicity, we’ll fit the model to the combined dataset. First, let’s have a look at the parameter estimates from our data. Since we’re using Bayesian parameter estimation, we’ll obtain a distribution of values for each parameter. We can use the median of the distribution as a point estimate and the 95% highest density interval as the range of plausible values.\n\n\n\nLet’s briefly review the parameters. We find an \\(\\alpha\\) around 0.65, capturing the typical diminishing returns of the value function, while the \\(\\lambda\\) of 2 reflects the expected loss-aversion. The \\(\\theta_1\\) value indicates that contestants initially set their reference point just below the current offer, while \\(\\theta_2\\) and \\(\\theta_3\\) suggest that the reference point is (partially) drawn towards the expected value of recent rounds and of the game overall. Finally, contestants estimate \\(\\rho\\) to be higher than it truly is, meaning they expect the offers to improve more quickly than they do.\nOk, great! Those parameters values look plausible, no major red flags there. This doesn’t really help us to understand what predictions the model will make, though. To achieve that, let’s plot the offers made by The Banker one more time—this time coloured according to the model’s predictions for the likelihood that the offer will be accepted. The Predicted Decision score here reflects the probability that the contestant will accept the offer, according to our model…\n\n\n\n\n\nYou’ll notice that these predictions look pretty reasonable – not too far from the pattern of decisions we saw in the data itself!\nThere are a few notable deviations from the predictions; these are likely driven by individual variability in the true underlying parameters. To address this, we would ideally be able to estimate the parameters for each contestant separately. Unfortunately, though, we don’t have enough data per contestant to do so—if we wanted to be able to fine-tune our model to each individual, we would need each contestant to play several rounds of the game.\nUntil next time…\nSo, there we have it, a comprehensive computational account of the decisions made by contestants in Deal or No Deal. What we can take away from this is that this game is not actually about “Beating the Banker”. Instead, it’s about holding your nerve until The Banker starts offering you something close to your subjective valuation of the remaining cases. Everyone’s subjective value calculation is slightly different—meaning there isn’t really a correct or incorrect way to play. If a contestant accepts an offer that’s way below the Expected Value, they might have miscalculated the Expected Value OR they might simply be extremely loss-averse.\nIn part 3, we’ll be turning the game on its head, embracing our ruthless capitalist and utilizing everything we’ve learned so far to answer the question: “How can The Banker minimize contestants’ winnings?”\n\n\n\nPost, Thierry, Martijn J van den Assem, Guido Baltussen, and Richard H Thaler. 2008. “Deal or No Deal? Decision Making Under Risk in a Large-Payoff Game Show.” American Economic Review 98 (March): 38–71. https://doi.org/10.1257/aer.98.1.38.\n\n\nTversky, Amos, and Daniel Kahneman. 1992. “Advances in Prospect Theory: Cumulative Representation of Uncertainty.” Journal of Risk and Uncertainty 5 (October): 297–323. https://doi.org/10.1007/BF00122574/METRICS.\n\n\nUsing the expected value from two rounds ago here is somewhat arbitrary—this could equally be the expected value from one round ago, for example. The main idea is that the reference point is drawn towards the expected value of some previous state. However, when testing the modelling approach, I tried substituting this for either (1) the expected value from one round ago or (2) the average expected value from the last two rounds and neither of these alternatives produced a better model fit.↩︎\nWe can estimate this using guideline parameters of \\(\\alpha = .88\\) and \\(\\lambda = 2.25\\) and by assuming Contestant 1 has a RP of $15,000 while Contestant 2 has a RP of $25,000. From these parameters, we can calculate the No Deal value—remembering that the higher the No Deal value is, the more the contestant would need to be offered to take the deal. Contestant 1 has a No Deal value of $14,735, while Contestant 2 has a No Deal value of $19,211. This means that Contestant 2 needs a significantly higher offer in order to be persuaded to take the deal.↩︎\n",
    "preview": "posts/2024-08-03-dondpart2/DoND_buzzer.png",
    "last_modified": "2024-11-03T18:18:23-05:00",
    "input_file": "dondpart2.knit.md",
    "preview_width": 1792,
    "preview_height": 1024
  },
  {
    "path": "posts/2024-08-01-dondpart1/",
    "title": "Deal or No Deal | Part 1: Beating the Banker",
    "description": "Everyone has heard of Deal or No Deal, but how does the show actually work? And more importantly, is it actually possible to \"Beat the Banker\"?",
    "author": [],
    "date": "2024-08-01",
    "categories": [],
    "contents": "\n\nContents\nWhat is Deal or No Deal?\nHow should Deal or No Deal be played?\nHow do the offers in Deal or No Deal work?\nSo, can we “Beat the Banker”?\n\n\n\nWhat is Deal or No Deal?\nYou’re probably aware of the TV gameshow Deal or No Deal – since its launch in 2005, it has been licensed in 80 different countries, airing over 600 episodes of the original US version alone. For those who need a reminder, in Deal or No Deal, a contestant is offered a selection of 26 briefcases each containing a different amount of money ranging from a single cent all the way up to 1 million dollars. Over the course of the game, they play a series of rounds in which they open some of the briefcases, eliminating those cases from the game. At the end of each round, “The Banker” offers the contestant a guaranteed cash value to quit the game. If the contestant declines, the game continues until only one case remains and the contestant wins the amount in that case.\nIt’s an incredibly simple format, but it also involves a series of crucial decisions all centred around the question:\n\nHow much should you be willing to accept from The Banker?\n\n\nIn this series of posts, I’ll be using a dataset of over 145 games1 of Deal or No Deal to explore the mechanics of the TV game show. In part 1, I’ll explore how the mysterious Banker produces their offers for the contestants to either accept or reject. In part 2, I’ll examine how those contestants make their decisions. And in part 3, I’ll use all of those insights to design the ultimate Banker.\nHow should Deal or No Deal be played?\nOn first consideration, the solution to Deal or No Deal is quite simple: you should never accept any offer that is less than the Expected Value of the remaining cases. When we say Expected Value, what we mean is the average amount in each of the remaining cases. For example, if you had two cases remaining, the 1-cent case and the 1 million dollar case, you should not be willing to accept anything less than $500,000 from The Banker because, on average, that is the amount that you will win.\nThe predicament, however, is that The Banker (i.e., the showrunners) also understands Expected Value and adjusts their offers accordingly. To really optimize our strategy, we’ll have to better understand how these offers are formulated…\nHow do the offers in Deal or No Deal work?\nBelow, I’ve plotted the offers made by The Banker in nearly 1000 rounds of Deal or No Deal played in three different countries2. The solid diagonal line indicates parity between the Expected Value and The Banker’s offer. You’ll notice that the offers are extremely variable, but almost never rise above the Expected Value.\n\n\n\nTo understand these offers, we first have to understand the motives of the showrunners. To make the show entertaining (and fill the 44-minute runtime), they do not want contestants to accept the first offer they receive. To make sure of this, they make the first offer extremely low. In fact, for the 225 contestants we are analyzing, the first round offer was an average of 8.52% of the Expected Value.\nHowever, it is also in the showrunners’ interests that not every contestant goes to the final round; there need to be some difficult decisions and suspense. To make sure of this, The Banker’s offer gets closer to the Expected Value with every round that passes. This is best demonstrated visually – let’s plot the offers again, but this time, we’ll give each offer a colour corresponding to the round in which it was made. You can use the legend to toggle each round on/off.\n\n\n\n\n\nNow we can start to see a trend emerging! The early-round offers are well below the line of parity, but as we get closer to round 9 (the final round) those offers start to approach the Expected Value of the remaining cases.\nWe can do a bit better than this, though – how exactly does The Banker determine what proportion of the Expected Value to offer each round? One suggestion made by Post et al. (2008) is that the offer for each round is a proportion (\\(b_r\\)) of the Expected Value that takes the proportion offered on the previous round (\\(b_{r-1}\\)) and increases it in progressively smaller steps until the final round is reached:\n\\[\\begin{equation}\n\\tag{1}\nb_r = b_{r-1} + (1 - b_{r-1})\\cdot\\rho^{9-(r-1)}\n\\end{equation}\\]\nHere, \\(0 < \\rho < 1\\) dictates the rate at which the proportion offered changes – the closer \\(\\rho\\) is to 1, the quicker the offers will approach 100% of the Expected Value…\nThere’s quite a strong assumption baked into this – that the proportion of the Expected Value offered each round is dependent on the proportion offered in the previous round. It’s equally plausible that, instead, the proportion offered follows a set trajectory, with random noise added independently each round. Post et al. don’t directly address this assumption, so we’ll have to do some digging of our own.\nBelow is the correlation between the proportion of the Expected Value offered in round \\(r\\) and the proportion of the Expected Value offered in the preceding round \\(r-1\\). To prevent scaling issues, I’ve z-scored the proportions in each round. If the offer is dependent on the previous round, there will be a positive correlation…\n\n\n\n…and that’s exactly what we see. Great! Let’s adopt Equation 1 from the Post et al. and replicate their results.\nIf we treat our data hierarchically, each country in our dataset produces a slightly different \\(\\rho\\) estimate. The US edition has a \\(\\rho\\) of 0.778, in the German edition, \\(\\rho =\\) 0.775, while in the Dutch, \\(\\rho =\\) 0.804.\nIgnoring the hierarchical structure, we get an aggregated estimate of \\(\\rho = .787\\). Let’s see how these estimates map onto our data. Here, we have the aggregate model predictions for each round shown as dashed lines. The predictions here clearly illustrate how the model correctly increases its predicted offers as the rounds progress.\n\n\n\nSo, can we “Beat the Banker”?\nLet’s summarize what we’ve learned so far…\nThe Banker’s offers are somewhat predictable. Each round, The Banker’s offer is some proportion of the Expected Value of the remaining cases. The offers seem to have some variability, whether through the addition of random noise or deliberate decisions made by the showrunners.\nConsecutive offers are not independent. The exact proportion of the Expected Value offered is a function of the proportion offered last round. That proportion increases as the rounds progress, starting with large increases before steadily plateauing.\n\nSo, what does this mean for us beating The Banker?\n\n\nWell, there are a few takeaway points:\n\n\nThe offers you get in the first few rounds will generally be terrible. The Banker’s early offers will be extreme lowballs that should be easily rejected.\nThe Banker will almost never offer you above the Expected Value of the remaining cases. You won’t be getting your money’s worth from The Banker until the very final rounds.\nGiven The Banker is almost never going to offer you the true value of your case: Does this mean that it is right for everyone to simply reject every offer and play until the final round? Well… not quite. While this strategy will earn you the biggest win on average, there will be a lot more variability in the final outcome than if you accept an earlier offer.\nTo quickly illustrate this, I’ve plotted the distributions of possible outcomes in each round of a game3. I’ve multiplied each outcome by the average proportion of the Expected Value offered by The Banker to illustrate how the offers evolve over the course of the game. You will notice that the average offer creeps up in the later rounds, as The Banker starts to make offers approaching the true Expected Value. However, the median offer starts to decrease in later rounds as the distribution skews to the right, leaving a higher chance of a very small offer than in round 4, for example…\n\n\n\nSo, it’s really up to the player to determine whether the higher average rewards available in later rounds are worth the additional variability in rewards!\nOnce we get past the idea of “Beating The Banker”, the question of whether an offer is subjectively substantial enough for you to accept it is a much more interesting one. It starts to invoke core principles of modern behavioural economics and decision science that allow this relatively simple game to produce its trademark difficult decisions.\nIn the next post, we’ll explore how the contestants make their decisions about whether to accept an offer.\n\n\n\nPost, Thierry, Martijn J van den Assem, Guido Baltussen, and Richard H Thaler. 2008. “Deal or No Deal? Decision Making Under Risk in a Large-Payoff Game Show.” American Economic Review 98 (March): 38–71. https://doi.org/10.1257/aer.98.1.38.\n\n\nThe data used in this post is licensed under the Creative Commons Attribution 4.0 International License. Modifications were made to the original materials for analysis purposes. Credit for this dataset should be given to the original authors: Thierry Post, Martijn van den Assem, Guido Baltussen, and Richard Thaler, and to the American Economic Association. The full dataset is available here.↩︎\nHere we’re working with data from the US, German, and Dutch editions of the show. For the sake of plotting, I’ve transformed the data from the European versions to dollar amounts peaking at 1 million dollars.↩︎\nI’m using the case values from the US edition here – the cases in the European editions are slightly different, but the same general principles apply.↩︎\n",
    "preview": "posts/2024-08-01-dondpart1/DoND_case2.png",
    "last_modified": "2024-10-10T14:27:07-04:00",
    "input_file": {},
    "preview_width": 1792,
    "preview_height": 1024
  }
]
