---
title: "Deal or No Deal | Part 1: Beating the Banker"
description: |
  Everyone has heard of Deal or No Deal, but how does the show actually work?
author:
  - name: Matthew Jiwa
    url: {}
date: 2024-08-01
preview: DoND_case2.png
output:
  distill::distill_article:
    self_contained: false
---

```{css, echo=FALSE}
img {
  max-width:100%;
  height: auto;
}

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(plyr)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(viridis)
library(plotly)
library(htmltools)
library(ggtext)

# Load
source(here::here("data", "DOND", "preProcess.R"))

blank_theme <- theme(
  axis.text.y = element_text(size = 10, colour = "black"),
  axis.text.x = element_text(size = 10, colour = "black"),
  axis.title = element_text(size = 11, colour = "black"),
  plot.title = element_text(size = 13, colour = "black"),
  panel.grid.major = element_blank(), 
  panel.grid.minor = element_blank(),
  panel.background = element_blank(),
  axis.line = element_line(colour = "black"),
  legend.text = element_text(size = 8, colour = "black"),
  legend.title = element_text(size = 9, colour = "black"),
  legend.margin=margin(t = 0, unit='cm'),
  legend.position = "none",
  panel.border = element_rect(colour = "black", fill=NA, linewidth=1)
)

```

### What is *Deal or No Deal*?

You're probably aware of the TV gameshow *Deal or No Deal* -- since its launch in 2005, it has been licensed in 80 different countries, airing over 600 episodes of the original US version alone. For those who need a reminder, in *Deal or No Deal*, a contestant is offered a selection of 26 briefcases each containing a different amount of money ranging from a single cent all the way up to 1 million dollars. Over the course of the game, they play a series of rounds in which they open some of the briefcases, eliminating those cases from the game. At the end of each round, "The Banker" offers the contestant a guaranteed cash value to quit the game. If the contestant declines, the game continues until only one case remains and the contestant wins the amount in that case.

It's an incredibly simple format, but it also involves a series of crucial decisions all centred around the question:

<center><span style="font-size: 18px;">*How much should you be willing to accept from The Banker?*</span></center>
<br>

In this series of posts, I'll be using a dataset of over 225 games^[The data used in this post is licensed under the [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/). Modifications were made to the original materials for analysis purposes. Credit for this dataset should be given to the original authors: Thierry Post, Martijn van den Assem, Guido Baltussen, and Richard Thaler, and to the American Economic Association. [The full dataset is available here](https://www.openicpsr.org/openicpsr/project/113232/version/V1/view;jsessionid=38766B84B8D203291DF7B953AF315EC6).
] of *Deal or No Deal* to explore the mechanics of the TV game show. In part 1, I'll explore how the mysterious Banker produces their offers for the contestants to either accept or reject. In part 2, I'll examine how those contestants make their decisions. And in part 3, I'll use all of those insights to design the ultimate Banker.

### So, can we actually "Beat the Banker"?

On first consideration, the solution to the game is quite simple: you should never accept any offer that is less than the *Expected Value* of the remaining cases. When we say Expected Value, what we mean is the average amount in each of the remaining cases. For example, if you had two cases remaining, the 1-cent case and the 1 million dollar case, you should not be willing to accept anything less than $500,000 from The Banker because, on average, that is the amount that you will win.

The predicament, however, is that The Banker (i.e., the showrunners) also understands Expected Value and adjusts their offers accordingly. To really optimize our strategy, we'll have to better understand how these offers are formulated...

### How do the offers in *Deal or No Deal* work?

Below, I've plotted the offers made by The Banker in nearly 1000 rounds of *Deal or No Deal* played in three different countries^[Here we're working with data from the US, German, and Dutch editions of the show. For the sake of plotting, I've transformed the data from the European versions to dollar amounts peaking at 1 million dollars.]. The solid diagonal line indicates parity between the Expected Value and The Banker's offer. You'll notice that the offers are extremely variable, but almost never rise above the Expected Value.


```{r plot_EVoffers, echo=FALSE, warning = FALSE, message=FALSE, fig.width = 4, fig.height = 4, fig.align = 'center', results = 'asis'}

plotData <- allData %>% 
  filter(
    !(edition %in% c("EXP1", "EXP2"))
  ) %>% 
  dplyr::rename(
    ExpectedValue = mean_N,
    OfferValue = offer_N
  )

# Plot
ggplot(plotData, aes(x = ExpectedValue, y = OfferValue)) +
  geom_point(alpha = 0.6) +
  # geom_point(data = plotData %>% filter(deal == 1), colour = "black") +
  geom_abline() +
  coord_equal() +
  labs(
    x = "Expected Value ($)",
    y = "Bank Offer ($)",
    colour = "Round Number"
  ) +
  scale_x_continuous(labels = scales::label_number(scale = 1e-3, suffix = "k")) +
  scale_y_continuous(labels = scales::label_number(scale = 1e-3, suffix = "k")) +
  blank_theme +
  theme(legend.position = "right",
        axis.text.y = element_text(size = 14, colour = "black"),
        axis.text.x = element_text(size = 14, colour = "black"),
        axis.title = element_text(size = 16, colour = "black"),
        plot.title = element_text(size = 18, colour = "black"),
        panel.grid.major = element_line(
          colour = "grey90",
          linewidth = .5
        )) # Top outlier is GE3.017


```

To understand these offers, we first have to understand the motives of the showrunners. To make the show entertaining (and fill the 44-minute runtime), they do **not** want contestants to accept the first offer they receive. To make sure of this, they make the first offer <u>extremely low</u>. In fact, for the 225 contestants we are analyzing, the first round offer was an average of `r round(mean(allData$EVperc[allData$round==1])*100, 2)`% of the Expected Value.

However, it is also in the showrunners' interests that not every contestant goes to the final round; there need to be some difficult decisions and suspense. To make sure of this, The Banker's offer gets closer to the Expected Value with every round that passes. This is best demonstrated visually -- let's plot the offers again, but this time, we'll give each offer a colour corresponding to the round in which it was made. You can use the legend to toggle each round on/off.

```{r plot_EVoffers2, echo=FALSE, warning = FALSE, message=FALSE, fig.align = 'center', fig.height = 6, fig.width = 6, results = 'asis'}

plotData <- allData %>% 
  filter(
    !(edition %in% c("EXP1", "EXP2"))
  ) %>% 
  dplyr::rename(
    ExpectedValue = mean_N,
    OfferValue = offer_N
  ) %>% 
  mutate(
    Round = factor(round)
  )

p <- ggplot(plotData, aes(x = ExpectedValue, y = OfferValue, colour = Round, 
                          text = paste("Expected Value: $", 
                                       formatC(ExpectedValue, format = "f", digits = 0, big.mark = ",",
                                               decimal.mark = ".", drop0trailing = TRUE),
                                       "<br>Offer Value: $", formatC(OfferValue, format = "f", 
                                                                     digits = 0, big.mark = ",",
                                                                     decimal.mark = ".", 
                                                                     drop0trailing = TRUE),
                                       "<br>Round: ", Round, sep = ""))) +
  geom_point(alpha = 0.6) +
  # geom_point(data = allData %>% filter(deal == 1), colour = "black") +
  geom_abline() +
  coord_equal() +
  labs(
    x = "Expected Value ($)",
    y = "Bank Offer ($)",
    colour = "Round"
  ) +
  scale_color_viridis_d() +  # Add viridis color scale
  scale_x_continuous(labels = scales::label_number(scale = 1e-3, suffix = "k")) +
  scale_y_continuous(labels = scales::label_number(scale = 1e-3, suffix = "k")) +
  blank_theme +
  theme(legend.position = "bottom",
        legend.text = element_text(size = 10, colour = "black"),
        legend.title = element_text(size = 11, colour = "black"),
        panel.grid.major = element_line(
          colour = "grey90",
          linewidth = .5
        )) # Top outlier is GE3.017

# Convert to interactive plot using plotly
plotly_plot <- ggplotly(p, tooltip = "text")
# Configure to remove the toolbar
plotly_plot <- plotly_plot %>% config(displayModeBar = FALSE)

# Enforce coord_equal limits and aspect ratio
plotly_plot <- plotly_plot %>%
  layout(
    legend = list(
      orientation = "h",  # Horizontal layout
      x = 0.5,            # Center the legend horizontally
      xanchor = "center", # Anchor point of the legend
      y = 1.12,           # Position the legend below the plot
      yanchor = "bottom"     # Anchor the legend from the top to push it downwards
    ),
    dragmode = FALSE,
    xaxis = list(
      scaleanchor = "y",  # Maintain equal scaling
      scaleratio = 1,     # Set the ratio to 1
      title = "Expected Value ($)"
    ),
    yaxis = list(
      title = "Bank Offer ($)"
    ),
    legend = list(title = list(text = "Round Number")),
    autosize = TRUE
  )

# Center the plot with fixed width and height
htmltools::browsable(
  htmltools::tagList(
    htmltools::tags$div(
      style = "display: flex; justify-content: center; align-items: center; height: auto !important; width: 400px; margin: auto; max-width: 100%",
      plotly_plot
    )
  )
)


# Load this in advance for use in the text
load(here::here("data", "DOND", "StanOffers", "Samples", "H_OfferModelX2_model_samples.Rdata"))

```

Now we can start to see a trend emerging! The early-round offers are well below the line of parity, but as we get closer to round 9 (the final round) those offers start to approach the Expected Value of the remaining cases.

We can do a bit better than this, though -- how exactly does The Banker determine what proportion of the Expected Value to offer each round? One suggestion made by Post et al. (2008)^[Full article available here: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=636508] is that the offer for each round is a proportion ($b_r$) of the Expected Value that takes the proportion offered on the previous round ($b_{r-1}$) and increases it in progressively smaller steps as it until the final round is reached:

\begin{equation}
\tag{1}
b_r = b_{r-1} + (1 - b_{r-1})\cdot\rho^{9-(r-1)}
\end{equation}

There's quite a strong assumption baked into this -- that the proportion of the Expected Value offered each round is dependent on the proportion offered in the previous round. It's equally plausible that, instead, the proportion offered follows a set trajectory, with random noise added independently each round. Post et al. don't directly address this assumption, so we'll have to do some digging of our own.

Below is the correlation between the proportion of the Expected Value offered in round $r$ and the proportion of the Expected Value offered in the preceding round $r-1$. To prevent scaling issues, I've z-scored the proportions in each round. If the offer is dependent on the previous round, there will be a positive correlation...

```{r plot_zScoreoffers, echo=FALSE, warning = FALSE, message=FALSE, fig.width = 12, fig.height = 4, fig.align = 'center', results = 'asis'}

plotData <- allData %>%
  filter(!(edition %in% c("EXP1", "EXP2"))) %>% 
  rename(
    ExpectedValue = mean_N,
    OfferValue = offer_N
  ) %>% 
  group_by(country, round) %>% 
  mutate(
    z_score_EVperc = (EVperc - mean(EVperc)) / sd(EVperc)
  ) %>%
  ungroup() %>%
  group_by(numID) %>%
  mutate(
    all_above_or_below_zero = all(z_score_EVperc > 0) | all(z_score_EVperc < 0)
  ) %>%
  ungroup()

plotDataLag <- plotData %>%
  group_by(numID) %>%
  arrange(numID, round) %>%
  mutate(lag_z_score_EVperc = lag(z_score_EVperc, 1)) %>%
  filter(!is.na(lag_z_score_EVperc)) %>% 
  rename(Country = country) %>% 
  ungroup()

cor_data <- plotDataLag %>%
  group_by(Country) %>%
  summarize(correlation = cor(lag_z_score_EVperc, z_score_EVperc, use = "complete.obs"),
            .groups = 'drop')

plotDataLag <- plotDataLag %>%
  left_join(cor_data, by = "Country")

# Plotting the lag plot
ggplot(plotDataLag, aes(x = lag_z_score_EVperc, y = z_score_EVperc)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  geom_text(aes(label = sprintf("r = %.2f", correlation), x = 3.5, y = -1.5, hjust = 1.1, vjust = 1.1),
            check_overlap = TRUE, data = unique(plotDataLag[, c("Country", "correlation")]),
            size = 6) +
  labs(x = "Z-Score in Round r-1", y = "Z-Score in Round r") +
  ggtitle("Lag Plot of Z-Scored Offer (Proportion of EV) to Check Autocorrelation") +
  scale_x_continuous(breaks = c(-2, 0, 2, 4)) +  # Setting custom x-ticks
  facet_wrap(~Country, labeller = label_both, ncol = 3) +
  blank_theme +
  theme(legend.position = "right",
        strip.text = element_text(size = 17, face = "bold"),
        axis.text.y = element_text(size = 17, colour = "black"),
        axis.text.x = element_text(size = 17, colour = "black"),
        axis.title = element_text(size = 19, colour = "black"),
        plot.title = element_text(size = 20, colour = "black"),
        panel.grid.major = element_line(
          colour = "grey90",
          linewidth = .5
        ))


```

...and that's exactly what we see. Great! Let's adopt Equation 1 from the Post et al. and replicate their results.

If we treat our data hierarchically, each country in our dataset produces a slightly different $\rho$ estimate. The US edition has a $\rho$ of `r round(colMeans(model_samples$Rho)[3], 3)`, in the German edition, $\rho =$ `r round(colMeans(model_samples$Rho)[1], 3)`, while in the Dutch, $\rho =$ `r round(colMeans(model_samples$Rho)[2], 3)`.

Ignoring the hierarchical structure, we get an aggregated estimate of $\rho = .787$. Let's see how these estimates map onto our data. Here, we have the aggregate model predictions for each round shown as dashed lines. The predictions here clearly illustrate how the model correctly increases its predicted offers as the rounds progress.


```{r plot_EVoffers3, echo=FALSE, warning = FALSE, message=FALSE, fig.width = 12, fig.height = 7, fig.align = 'center', results = 'asis'}

## Using the hierarchical model

plotData <- allData %>% 
  dplyr::rename(
    Round = round
  ) %>% 
  filter(
    !(edition %in% c("EXP1", "EXP2")) & Round != 1
  ) %>% 
  mutate(
    predEVperc = colMeans(model_samples$output_pred),
    predDiff = predEVperc - EVperc
  )

corPlot <- ggplot(data = plotData, aes(x = EVperc, y = predEVperc)) +
  geom_point(size = .5) +
  geom_abline() +
  xlim(0,1.95) +
  ylim(0,1.95) +
  labs(
    x = "Offer (Proportion of Expected Value)",
    y = "Predicted Offer (Proportion of Expected Value)"
  ) +
  annotate("text", label = paste("r = ", round(cor.test(plotData$EVperc, plotData$predEVperc, 
                                                        paired = T)$estimate,
                                               2), sep = ""), x = .5, y = 1.5, size = 6) +
  coord_equal() +
  blank_theme


## Now using the aggregated model

load(here::here("data", "DOND", "StanOffers", "Samples", "OfferModelX2_model_samples.Rdata"))

plotData <- allData %>% 
  dplyr::rename(
    Round = round
  ) %>% 
  filter(
    !(edition %in% c("EXP1", "EXP2")) & Round != 1
  ) %>% 
  mutate(
    predEVperc = colMeans(model_samples$output_pred),
    predDiff = predEVperc - EVperc
  )

EVpercData <- plotData %>% 
  filter(
    Round != 1
  ) %>% 
  group_by(Round) %>% 
  summarise(meanPredEVperc = mean(predEVperc)) %>% 
  ungroup()

predPlot <- ggplot(plotData, aes(x = mean_N, y = offer_N, colour = factor(Round))) +
    geom_point() +
    scale_color_viridis_d() +  # Add viridis color scale
    geom_abline() +
    geom_abline(aes(intercept = 0, slope = meanPredEVperc), colour = "black", 
                data = EVpercData, linetype = "dashed") +
    coord_equal() +
    labs(
        x = "Expected Value ($)",
        y = "Bank Offer ($)",
        colour = "Round Number"
    ) +
    scale_x_continuous(labels = scales::label_number(scale = 1e-3, suffix = "k")) +
    scale_y_continuous(labels = scales::label_number(scale = 1e-3, suffix = "k")) +
    facet_wrap(~Round, labeller = label_both, ncol = 4) +
    blank_theme +
    theme(legend.position = "none",
          strip.text = element_text(size = 20, face = "bold"),
          axis.text.y = element_text(size = 18, colour = "black"),
          axis.title = element_text(size = 21, colour = "black"),
          axis.text.x = element_text(size = 18, colour = "black", angle = 90, vjust = 0.5, hjust = 1),
          panel.grid.major = element_line(
              colour = "grey90",
              linewidth = .5
          )) # Top outlier is GE3.017

errorPlot <- ggplot(data = plotData) + 
  geom_histogram(aes(x = predDiff, fill = factor(Round))) +
  scale_fill_viridis_d() +
  geom_vline(aes(xintercept = 0), linetype = "dashed") +
  xlim(-1, 1) +
  ylim(0, 75) +
  labs(
    y = "Count", 
    x = "Prediction Error"
  ) +
  facet_wrap(~Round, labeller = label_both, ncol = 4) +
  coord_fixed(ratio = .0266) +
  blank_theme +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
        panel.grid.major = element_line(
            colour = "grey90",
            linewidth = .5
        ))

show(predPlot)

# show(errorPlot)

```

To summarize:

- **The Banker's offers are somewhat predictable.** Each round, The Banker makes an offer that is some proportion of the Expected Value of the remaining cases. The offers seem to have some variability, whether through the addition of random noise 
- **Consecutive offers are not independent.** The exact proportion of the Expected Value offered is a function of the proportion offered last round. That proportion increases as the rounds progress.


<center><span style="font-size: 18px;">*So what does this mean for us beating The Banker?*</span></center>
<center><span style="font-size: 17px;">Well, there are a few takeaway points:</span></center>
<br>

1) **The offers you get in the first few rounds will generally be terrible.** The Banker's early offers will be extreme lowballs that should be easily rejected.
2) **The Banker will almost never offer you above the Expected Value of the remaining cases.** You won't be getting your money's worth from The Banker until the very final rounds.

Given The Banker is almost never going to offer you the true value of your case: **Does this mean that it is right for everyone to simply reject every offer and play until the final round?** Well... not quite. While this strategy will earn you the biggest win on average, there will be a lot more variability in the final outcome than if you accept an earlier offer. To quickly illustrate this, I've plotted the distributions of possible outcomes in each round of a game^[I'm using the case values from the US edition here -- the cases in the European editions are slightly different, but the same general principles apply.]. I've multiplied each outcome by the average proportion of the Expected Value offered by The Banker to illustrate how the offers evolve over the course of the game. You will notice that the <span style='font-size:14pt; color:#1d0dff;'><b>average offer</b></span> creeps up in the later rounds, as The Banker starts to make offers approaching the true Expected Value. However, the <span style='font-size:14pt; color:#cf3400;'><b>median offer</b></span> starts to decrease in later rounds as the distribution skews to the right, leaving a higher chance of a very small offer than in round 4, for example...

```{r plot_possibleOffers, echo=FALSE, warning = FALSE, message=FALSE, fig.width = 8, fig.height = 5, fig.align = 'center', results = 'asis'}

## Saved the output of all of this

# rawData_sheetNames <- excel_sheets(here::here("data", "DOND", "RawData", "20060455_Data.xls"))
# 
# rawData_allSheets <- lapply(rawData_sheetNames, function(sheet) {
#   read_excel(here::here("data", "DOND", "RawData", "20060455_Data.xls"), sheet = sheet)
# })
# 
# names(rawData_allSheets) <- rawData_sheetNames
# rawData_allSheets <- rawData_allSheets[names(rawData_allSheets) == "US"]
# 
# dataset <- rawData_allSheets[[1]]
# possibleOutcomes_str <- colnames(dataset)[12:ncol(dataset)]
# allCases <- as.numeric(sub("\\.\\.\\..*", "", possibleOutcomes_str))
# 
# # Function to calculate sample means for all combinations
# calculate_means <- function(data, sample_size) {
#   means <- combn(data, sample_size, FUN = mean, simplify = TRUE)
#   data.frame(SampleSize = sample_size, Mean = means)
# }
# 
# # Calculate means for different sample sizes
# sample_sizes <- c(20, 15, 11, 8, 6, 5, 4, 3, 2)  # Define the sizes as per your description
# results <- lapply(sample_sizes, calculate_means, data = allCases)
# mean_data <- do.call(rbind, results)   # Combine all data frames into one
# 
# # Create a mapping from SampleSize to Round
# sample_size_to_round <- setNames(seq_along(sample_sizes), sample_sizes)
# 
# # Add 'Round' column to mean_data
# mean_data <- mean_data %>%
#   mutate(Round = sample_size_to_round[as.character(SampleSize)])
# 
# weight_df <- allData %>% 
#   group_by(round) %>% 
#   rename(Round = round) %>% 
#   summarise(weight = mean(EVperc))
# 
# mean_data <- mean_data %>%
#   left_join(weight_df, by = "Round") %>%
#   mutate(WeightedMean = Mean * weight)  # Calculate the weighted mean
# 
# stats_per_round <- mean_data %>%
#   group_by(Round) %>%
#   summarise(
#     AvgWeightedMean = mean(WeightedMean),
#     MedianWeightedMean = median(WeightedMean)
#   )
# 
# # Join back to mean_data for consistent faceting
# mean_data <- left_join(mean_data, stats_per_round, by = "Round")
# 
# save("mean_data", file=here::here("data", "DOND", "allSampleData.Rdata"))

load(here::here("data", "DOND", "allSampleData.Rdata"))

stats_per_round <- mean_data %>%
  group_by(Round) %>%
  summarise(
    AvgWeightedMean = mean(WeightedMean),
    MedianWeightedMean = median(WeightedMean)
  )

expectedOfferPlot <- ggplot(mean_data, aes(x = WeightedMean)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "black") +
  geom_vline(data = stats_per_round, aes(xintercept = AvgWeightedMean), 
             linetype = "dashed", color = "#1d0dff", size = .75) +
  geom_vline(data = stats_per_round, aes(xintercept = MedianWeightedMean), 
             linetype = "twodash", color = "#cf3400", size = .75) +
  facet_wrap(~Round, labeller = label_both, scales = "free_y") +
  labs(title = "Distribution of Possible Banker Offers Each Round",
       x = "Expected Offer ($)",
       y = "Frequency",
       subtitle = "<span style='font-size:14pt'>Lines indicate the 
    <span style='color:#1d0dff;'><b>Mean</b></span> and
    <span style='color:#cf3400;'><b>Median</b></span> of the expected offer
    </span>"
    ) +
  scale_x_continuous(labels = scales::label_number(scale = 1e-3, suffix = "k")) +
  blank_theme +
  theme(
    strip.text = element_text(size = 13, face = "bold"),
    axis.text.y = element_text(size = 13, colour = "black"),
    axis.text.x = element_text(size = 13, colour = "black"),
    axis.title = element_text(size = 16, colour = "black"),
    plot.title = element_text(size = 17, colour = "black"),
    plot.subtitle = element_markdown(lineheight = 1.1),
  )

show(expectedOfferPlot)

```


So, it's really up to the player to determine whether the higher average rewards available in later rounds are worth the additional variability in rewards!

Once we get past the idea of "Beating The Banker", the question of whether an offer is *subjectively* substantial enough for you to accept it is a much more interesting one. It starts to invoke core principles of modern behavioural economics and decision science that allow this relatively simple game to produce its trademark difficult decisions.

In the next post, we'll explore how the contestants make their decisions about whether to accept an offer.

